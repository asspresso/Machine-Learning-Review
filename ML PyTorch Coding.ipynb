{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393757cf-8ec9-4726-977d-936340c44738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f992cb-950e-4b66-a3fb-e3a34bbbe56b",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80423b8-faa0-4b35-97cc-17fb5c0a0ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# close form\n",
    "# (XTX)-1XTY\n",
    "def linear_regression_close(X, y):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return np.round(theta, 4).flatten().tolist()\n",
    "\n",
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 2, 3]\n",
    "print(linear_regression_close(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd50c688-5911-4e06-9091-4237728be346",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1107, 0.9513]\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "def linear_regression_gradient(X, y, alpha=0.01, iterations=1000):\n",
    "    X = np.array(X) \n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1)) \n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = X.T @ errors / m\n",
    "        theta -= alpha * gradient\n",
    "    return np.round(theta, 4).flatten().tolist()\n",
    "\n",
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 2, 3]\n",
    "print(linear_regression_gradient(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9843a-f27a-4b91-a71e-b7e1d39696be",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e786f3d4-97f3-4a5b-ae63-64d26ff03369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2529, 0.1954]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def logistic_regression(X, y, alpha=0.01, iterations=1000):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        z = X @ theta\n",
    "        predictions = sigmoid(z)\n",
    "        errors = predictions - y\n",
    "        gradients = X.T @ errors / m\n",
    "        theta -= alpha * gradients\n",
    "    return np.round(theta, 4).flatten().tolist()\n",
    "\n",
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 0, 1]\n",
    "print(logistic_regression(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a589a57-4a0d-414f-b5f3-228f67102dcd",
   "metadata": {},
   "source": [
    "# gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "438568ec-f2cc-48d0-a580-873425d25af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(n, x):\n",
    "    return (n - x ** 2) ** 2\n",
    "\n",
    "def gradient(n, x):\n",
    "    return -4 * x * (n - x ** 2)\n",
    "\n",
    "def gradient_descent(n, lr=0.01):\n",
    "    x = n \n",
    "    \n",
    "    if n > 1000: lr *= 10\n",
    "    \n",
    "    for i in range(1000):\n",
    "        loss = loss_fn(n, x)\n",
    "        if abs(loss) < 1e-10: \n",
    "            break\n",
    "        grad = gradient(n, x)\n",
    "        grad = max(min(grad, 100), -100)\n",
    "        x -= grad * lr\n",
    "        \n",
    "    return x\n",
    "\n",
    "gradient_descent(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6134f3c-9792-48f4-b451-1ca87d0f6a61",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae6ec431-6c27-4b4d-9850-94c727762ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0], [10.0, 2.0]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance(a, b):\n",
    "    return np.sum((a - b) ** 2) ** 0.5\n",
    "\n",
    "def k_means(points: list[tuple[float, float]],\n",
    "            k: int, \n",
    "            initial_centroids: list[tuple[float, float]], \n",
    "            max_iterations: int) -> list[tuple[float, float]]:\n",
    "    points = np.array(points)\n",
    "    centroids = np.array(initial_centroids)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        assignments = []\n",
    "        for point in points:\n",
    "            distances = np.array([distance(point, centroid) for centroid in centroids])\n",
    "            assignments.append(np.argmin(distances))\n",
    "        assignments = np.array(assignments)\n",
    "\n",
    "        new_centroids = []\n",
    "        for cluster in range(k):\n",
    "            subset = points[assignments == cluster]\n",
    "            if len(subset) > 0:\n",
    "                new_centroids.append(subset.mean(axis=0))\n",
    "            else:\n",
    "                new_centroids.append(centroids[cluster])\n",
    "        new_centroids = np.array(new_centroids)\n",
    "\n",
    "        if np.all(new_centroids == centroids):\n",
    "            break\n",
    "        else:\n",
    "            centroids = new_centroids\n",
    "    \n",
    "    return np.round(centroids, 4).tolist()\n",
    "    \n",
    "    \n",
    "points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n",
    "k = 2\n",
    "initial_centroids = [(1, 1), (10, 1)]\n",
    "max_iterations = 10\n",
    "k_means(points, k, initial_centroids, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340ab46-a348-4081-88cb-8d72fde247b9",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f122f6-3b75-4b84-8586-8eb8d839097b",
   "metadata": {},
   "source": [
    "n = number of heads\n",
    "\n",
    "B = batch size\n",
    "\n",
    "L = sequence length\n",
    "\n",
    "D = hidden dimension\n",
    "\n",
    "d = head dimension = D / n\n",
    "\n",
    "X: [B, L, D]\n",
    "\n",
    "Q, K, V: [B, n, L, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f3327ae-c360-41f1-89e0-a7d7c946e765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6604769, 2.6604769],\n",
       "       [2.3395231, 3.3395231]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single head attention\n",
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    return Q, K, V\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[1]\n",
    "    scores = Q @ K.T / np.sqrt(d_k)\n",
    "    score_max = np.max(scores, axis=1, keepdims=True)\n",
    "    weights = np.exp(scores - score_max) / np.sum(np.exp(scores - score_max), axis=1, keepdims=True)\n",
    "    attention = weights @ V\n",
    "    return attention\n",
    "\n",
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "self_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccc09f89-f669-4c5e-a0fd-957c95fc8f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_head_attention(Q, K, V, n_heads):\n",
    "    d_model = Q.shape[1]\n",
    "    assert d_model % n_heads == 0\n",
    "    d_k = d_model // n_heads\n",
    "    \n",
    "    Q_reshape = Q.reshape(Q.shape[0], n_heads, d_k).transpose(1, 0, 2)\n",
    "    K_reshape = K.reshape(K.shape[0], n_heads, d_k).transpose(1, 0, 2)\n",
    "    V_reshape = V.reshape(V.shape[0], n_heads, d_k).transpose(1, 0, 2)\n",
    "    \n",
    "    attentions = []\n",
    "    for i in range(n_heads):\n",
    "        attn = self_attention(Q_reshape[i], K_reshape[i], V_reshape[i])\n",
    "        attentions.append(attn)\n",
    "    \n",
    "    attention_output = np.concatenate(attentions, axis=1)\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cd25352-926a-4518-ab3c-105eab9f7dce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]]\n"
     ]
    }
   ],
   "source": [
    "m, n = 4, 4 \n",
    "n_heads = 2 \n",
    "np.random.seed(42) \n",
    "X = np.arange(m*n).reshape(m,n) \n",
    "X = np.random.permutation(X.flatten()).reshape(m, n) \n",
    "W_q = np.random.randint(0,4,size=(n,n)) \n",
    "W_k = np.random.randint(0,5,size=(n,n)) \n",
    "W_v = np.random.randint(0,6,size=(n,n)) \n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "print(multi_head_attention(Q, K, V, n_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40956f-351f-466c-af3f-4c5fdaf13e57",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a7cc8123-b12c-443a-91ab-d74dcc1f4b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ef18ea6-9f78-46ca-b837-3b701f668299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    total_count = len(labels)\n",
    "    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in label_counts.values())\n",
    "    return entropy\n",
    "\n",
    "def calculate_info_gain(examples, attr, target_attr):\n",
    "    total_entropy = calculate_entropy([example[target_attr] for example in examples])\n",
    "    values = set(example[attr] for example in examples)\n",
    "    attr_entropy = 0\n",
    "    for value in values:\n",
    "        value_subset = [example[target_attr] for example in examples if example[attr] == value]\n",
    "        value_entropy = calculate_entropy(value_subset)\n",
    "        attr_entropy += (len(value_subset) / len(examples)) * value_entropy\n",
    "    return total_entropy - attr_entropy\n",
    "\n",
    "def majority_class(examples, target_attr):\n",
    "    labels = [example[target_attr] for example in examples]\n",
    "    return max(labels, key=lambda x: labels.count(x))\n",
    "\n",
    "def learn_decision_tree(examples, attributes, target_attr):\n",
    "    if not examples: return \"no examples\"\n",
    "    if all(example[target_attr] == examples[0][target_attr] for example in examples):\n",
    "        return examples[0][target_attr]\n",
    "    if not attributes:\n",
    "        return majority_class(examples, target_attr)\n",
    "    \n",
    "    gains = {attr: calculate_info_gain(examples, attr, target_attr) for attr in attributes}\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "    tree = {best_attr: {}}\n",
    "    \n",
    "    for value in set(example[best_attr] for example in examples):\n",
    "        subset = [example for example in examples if example[best_attr] == value]\n",
    "        new_attributes = attributes.copy()\n",
    "        new_attributes.remove(best_attr)\n",
    "        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n",
    "        tree[best_attr][value] = subtree\n",
    "        \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b71a8b42-37e9-4074-b97f-d5ee9e1cad1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Outlook': {'Overcast': 'Yes', 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}}}\n"
     ]
    }
   ],
   "source": [
    "print(learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0968e7-3eee-46c9-8d70-44c483396f77",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafa031f-3795-45ca-a5ff-6027df9688bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0188  0.7675]\n",
      " [ 0.3687  0.0898]\n",
      " [-0.8084 -0.2955]\n",
      " [ 0.4585 -0.5618]]\n"
     ]
    }
   ],
   "source": [
    "def pca(data, k):\n",
    "    n_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "    covariance_matrix = np.cov(n_data, rowvar=False)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    idx = np.argsort(-eigenvalues)\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    principal_components = eigenvectors[:, :k]\n",
    "    return np.round(principal_components, 4)\n",
    "\n",
    "print(pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348923e-7b68-4b3e-a0e1-2bc3795f8f0f",
   "metadata": {},
   "source": [
    "## torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2f3ffb3-91de-45d0-8589-727da635d786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb94fa-88bb-4db0-8d1a-59fa7915cc4b",
   "metadata": {},
   "source": [
    "## layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f879acb-62b9-4bbb-a405-415039b0d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, beta, gamma):\n",
    "    x_mean = np.mean(x, axis=1, keepdims=True)\n",
    "    x_std = np.std(x, axis=1, keepdims=True)\n",
    "    normed_x = (x - x_mean) / (x_std + 1e-8)\n",
    "    normed_x = gamma * normed_x + beta\n",
    "    return normed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d7eb7-c982-47aa-bdb0-fda6bff23342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.o_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        batch_size = hidden_state.size()[0]\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "        query = split_heads(query, batch_size)\n",
    "        key = split_heads(key, batch_size)\n",
    "        value = split_heads(value, batch_size)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        \n",
    "        if attention_mask != None:\n",
    "            attention_scores += attention_mask * -1e9\n",
    "            \n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "        output = self.o_linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293287f-f3e6-414e-a3a9-ef8811e59e1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98725a6b-bc8c-4bd4-8106-afb5cd2c3940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
